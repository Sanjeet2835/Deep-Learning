{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b3ec6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4acc489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d108f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device using:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device using: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d24a4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,),(0.5))])\n",
    "\n",
    "train_data_ = datasets.FashionMNIST(root='./data', train = True, download=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST(root='./data', train = False, download=True, transform=transform)\n",
    "\n",
    "train_len = int(0.9*len(train_data_))\n",
    "val_len = len(train_data_)-train_len\n",
    "train_data, val_data = random_split(train_data_, (train_len, val_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "be0ca974",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True,\n",
    "                              pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False, \n",
    "                            pin_memory=True, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False,\n",
    "                             pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "96de883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class first_cnn(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        #cnn\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(num_features, 32, kernel_size = 3, padding='same'),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3ca6b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fashion_CNN = first_cnn(1).to(device)\n",
    "Loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Fashion_CNN.parameters(), lr = 0.01)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=4, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8eea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9 | Val Accuracy: 89.4833% | Train Accuracy: 84.9981% | Counter: 0 | Train Loss: 0.4273 | Val Loss: 0.2831\n",
      "Epoch 2/9 | Val Accuracy: 90.9167% | Train Accuracy: 89.5889% | Counter: 0 | Train Loss: 0.2940 | Val Loss: 0.2478\n",
      "Epoch 3/9 | Val Accuracy: 91.7667% | Train Accuracy: 90.8963% | Counter: 0 | Train Loss: 0.2564 | Val Loss: 0.2374\n",
      "Epoch 4/9 | Val Accuracy: 92.2833% | Train Accuracy: 91.8926% | Counter: 0 | Train Loss: 0.2257 | Val Loss: 0.2173\n",
      "Epoch 5/9 | Val Accuracy: 92.0833% | Train Accuracy: 92.6889% | Counter: 1 | Train Loss: 0.2055 | Val Loss: 0.2220\n",
      "Epoch 6/9 | Val Accuracy: 92.6000% | Train Accuracy: 93.4037% | Counter: 0 | Train Loss: 0.1839 | Val Loss: 0.2062\n",
      "Epoch 7/9 | Val Accuracy: 91.7833% | Train Accuracy: 94.0463% | Counter: 1 | Train Loss: 0.1649 | Val Loss: 0.2351\n",
      "Epoch 8/9 | Val Accuracy: 92.4500% | Train Accuracy: 94.7407% | Counter: 2 | Train Loss: 0.1461 | Val Loss: 0.2318\n",
      "Epoch 9/9 | Val Accuracy: 92.7000% | Train Accuracy: 95.1185% | Counter: 3 | Train Loss: 0.1328 | Val Loss: 0.2290\n"
     ]
    }
   ],
   "source": [
    "epochs = 9\n",
    "es_patience = 7\n",
    "counter = 0\n",
    "best_val_loss = float('inf')\n",
    "delta = 1e-4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    Fashion_CNN.train()\n",
    "    total_loss=0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = Fashion_CNN(images)\n",
    "        loss = Loss(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #train_accuracy logic\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        #loss\n",
    "        total_loss += loss.item()*labels.size(0) \n",
    "    avg_train_accuracy = 100 * correct / total\n",
    "    avg_train_loss = total_loss/total\n",
    "\n",
    "    #Validation\n",
    "    Fashion_CNN.eval()\n",
    "    total_val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = Fashion_CNN(images)\n",
    "            \n",
    "            #val loss logic\n",
    "            batch_loss = Loss(outputs, labels).item() #returns mean loss per sample\n",
    "            total_val_loss += batch_loss * labels.size(0)  # total loss = bl1*32+bl2*32+bl3*16\n",
    "            \n",
    "            \n",
    "    #Note: We could also take the mean of the average loss from all batches, but this would be inaccurate if the last batch has fewer samples than the others.\n",
    "            #val accuracy logic\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_total += labels.size(0) #number of samples. Its generally equal to defined batch_size(=64) but last batch can be smaller. eg 32 32 16. 32+32+16=80\n",
    "            \n",
    "    avg_val_loss = total_val_loss/val_total #loss per sample\n",
    "    lr_scheduler.step(avg_val_loss)  #schedular decides whether to change lr or not, epochwise, not batchwise\n",
    "        \n",
    "    avg_val_accuracy = 100 * val_correct / val_total #accuracy per sample \n",
    "    \n",
    "\n",
    "    #Early Stopping Check\n",
    "    if avg_val_loss < best_val_loss-delta:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save(Fashion_CNN.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        counter+=1\n",
    "        if counter>=es_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Val Accuracy: {avg_val_accuracy:.4f}% | Train Accuracy: {avg_train_accuracy:.4f}% | Counter: {counter} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "90945a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.16%\n"
     ]
    }
   ],
   "source": [
    "Fashion_CNN.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device) \n",
    "        outputs = Fashion_CNN(images) \n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
